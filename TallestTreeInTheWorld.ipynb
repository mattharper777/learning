{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMG3zxRT/UC+o4/p3Apx3Tu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mattharper777/learning/blob/main/TallestTreeInTheWorld.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multi-Agent LLM Deliberation System\n",
        "A notebook for running multiple LLMs with opposing personas to analyze topics\n",
        "Optimized for Google Colaboratory"
      ],
      "metadata": {
        "id": "KRrPoROPuQat"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RbxCDXrds48I",
        "outputId": "17c479cd-5de1-45dd-cc53-93df8510d17e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ“ Dependencies installed and loaded successfully\n",
            "âœ“ Ready to configure API keys in the next cell\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# CELL 1: Install Dependencies\n",
        "# =============================================================================\n",
        "\"\"\"\n",
        "Run this cell first to install required packages.\n",
        "This will take about 30-60 seconds.\n",
        "\"\"\"\n",
        "\n",
        "!pip install -q litellm pandas numpy tabulate openai anthropic google-generativeai\n",
        "\n",
        "import litellm\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from typing import List, Dict, Any\n",
        "import json\n",
        "import time\n",
        "from collections import Counter\n",
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "# Suppress litellm warnings\n",
        "litellm.suppress_debug_info = True\n",
        "\n",
        "print(\"âœ“ Dependencies installed and loaded successfully\")\n",
        "print(\"âœ“ Ready to configure API keys in the next cell\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# CELL 2: Configure API Keys (SECURE METHOD)\n",
        "# =============================================================================\n",
        "\"\"\"\n",
        "SECURE API KEY INPUT FOR GOOGLE COLAB\n",
        "\n",
        "This cell provides secure input for your API keys. They won't be visible after entry.\n",
        "You only need keys for the providers you want to use.\n",
        "\n",
        "Get API keys from:\n",
        "- OpenAI: https://platform.openai.com/api-keys\n",
        "- Anthropic: https://console.anthropic.com/\n",
        "- Google AI: https://makersuite.google.com/app/apikey\n",
        "\n",
        "OPTION 1: Use Google Colab Secrets (Recommended for repeated use)\n",
        "1. Click the key icon (ðŸ”‘) in the left sidebar\n",
        "2. Add secrets named: OPENAI_API_KEY, ANTHROPIC_API_KEY, GOOGLE_API_KEY\n",
        "3. Enable \"Notebook access\" for each\n",
        "4. Uncomment the \"Using Colab Secrets\" section below\n",
        "\n",
        "OPTION 2: Enter keys manually each time (Secure, one-time use)\n",
        "The default option below - keys won't be saved\n",
        "\"\"\"\n",
        "\n",
        "# Choose your method by uncommenting ONE of the following sections:\n",
        "\n",
        "# --- METHOD 1: Using Colab Secrets (Uncomment these 4 lines) ---\n",
        "from google.colab import userdata\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
        "os.environ[\"ANTHROPIC_API_KEY\"] = userdata.get('ANTHROPIC_API_KEY')\n",
        "os.environ[\"GOOGLE_API_KEY\"] = userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "# --- METHOD 2: Manual Entry (Default - Currently Active) ---\n",
        "#print(\"ðŸ” Enter your API keys securely (input will be hidden)\")\n",
        "#print(\"Press Enter to skip any provider you don't want to use\\n\")\n",
        "\n",
        "#openai_key = getpass(\"OpenAI API Key (or press Enter to skip): \")\n",
        "#anthropic_key = getpass(\"Anthropic API Key (or press Enter to skip): \")\n",
        "#google_key = getpass(\"Google API Key (or press Enter to skip): \")\n",
        "\n",
        "# The following lines are removed as they refer to 'openai_key', 'anthropic_key', 'google_key' which are not defined\n",
        "# when using Colab Secrets, and are redundant if manually entered.\n",
        "# if openai_key:\n",
        "#     os.environ[\"OPENAI_API_KEY\"] = openai_key\n",
        "# if anthropic_key:\n",
        "#     os.environ[\"ANTHROPIC_API_KEY\"] = anthropic_key\n",
        "# if google_key:\n",
        "#     os.environ[\"GOOGLE_API_KEY\"] = google_key\n",
        "\n",
        "# Configure which models to use based on available keys from environment variables\n",
        "MODELS = []\n",
        "\n",
        "if os.environ.get(\"OPENAI_API_KEY\"):\n",
        "    MODELS.append(\"gpt-4o-mini\")  # Cost-effective OpenAI model\n",
        "    # MODELS.append(\"gpt-4o\")  # Uncomment for more powerful (but expensive) model\n",
        "\n",
        "if os.environ.get(\"ANTHROPIC_API_KEY\"):\n",
        "    MODELS.append(\"claude-3-5-haiku-20241022\")  # Fast and efficient\n",
        "    # MODELS.append(\"claude-3-5-sonnet-20241022\")  # Uncomment for more powerful model\n",
        "\n",
        "if os.environ.get(\"GOOGLE_API_KEY\"):\n",
        "    MODELS.append(\"gemini-1.5-flash\")  # Fast and free tier available\n",
        "    # MODELS.append(\"gemini-1.5-pro\")  # Uncomment for more powerful model\n",
        "\n",
        "if not MODELS:\n",
        "    print(\"\\nâš ï¸  WARNING: No API keys provided! Please run this cell again with at least one API key.\")\n",
        "else:\n",
        "    print(f\"\\nâœ“ Configured {len(MODELS)} models:\")\n",
        "    for model in MODELS:\n",
        "        print(f\"  â€¢ {model}\")\n",
        "    print(\"\\nâœ“ Ready to proceed to the next cell!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cmx49HfKuOXk",
        "outputId": "52ef3c00-eb41-410e-bbd6-4212373f14e5"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "âœ“ Configured 3 models:\n",
            "  â€¢ gpt-4o-mini\n",
            "  â€¢ claude-3-5-haiku-20241022\n",
            "  â€¢ gemini-1.5-flash\n",
            "\n",
            "âœ“ Ready to proceed to the next cell!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# CELL 3: Define the Topic and Personas\n",
        "# =============================================================================\n",
        "\"\"\"\n",
        "Define what topic you want the agents to debate and what personas they should adopt.\n",
        "\"\"\"\n",
        "\n",
        "# The topic/question for analysis\n",
        "TOPIC = \"What is the tallest tree that ever existed?\"\n",
        "\n",
        "# Define opposing personas\n",
        "PERSONAS = {\n",
        "    \"optimist\": {\n",
        "        \"description\": \"An enthusiastic advocate who sees the benefits and opportunities\",\n",
        "        \"instruction\": \"Argue strongly in favor of this position. Focus on benefits, opportunities, and positive outcomes.\"\n",
        "    },\n",
        "    \"skeptic\": {\n",
        "        \"description\": \"A critical skeptic who identifies risks and problems\",\n",
        "        \"instruction\": \"Argue against this position. Focus on risks, downsides, and potential negative consequences.\"\n",
        "    },\n",
        "    \"pragmatist\": {\n",
        "        \"description\": \"A balanced, practical analyst\",\n",
        "        \"instruction\": \"Provide a balanced, practical analysis. Consider both benefits and challenges realistically.\"\n",
        "    }\n",
        "}\n",
        "\n",
        "print(f\"âœ“ Topic: {TOPIC}\")\n",
        "print(f\"âœ“ Personas: {list(PERSONAS.keys())}\")"
      ],
      "metadata": {
        "id": "cLLe9P6QvAgn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad63e9e6-c736-46e0-c7c6-11fa5ca99bb4"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ“ Topic: What is the tallest tree that ever existed?\n",
            "âœ“ Personas: ['optimist', 'skeptic', 'pragmatist']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def call_llm(model: str, prompt: str, max_retries: int = 3) -> str:\n",
        "    \"\"\"Call an LLM with retry logic.\"\"\"\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            if model.startswith(\"gemini\"):\n",
        "                api_key_to_use = os.environ.get(\"GOOGLE_API_KEY\")\n",
        "                if not api_key_to_use:\n",
        "                    return \"ERROR: GOOGLE_API_KEY not found in environment variables.\"\n",
        "\n",
        "                # Explicitly set litellm's API key for Gemini models\n",
        "                litellm.api_key['gemini'] = api_key_to_use\n",
        "\n",
        "                response = litellm.completion(\n",
        "                    model=model,\n",
        "                    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "                    temperature=0.7\n",
        "                )\n",
        "            else:\n",
        "                response = litellm.completion(\n",
        "                    model=model,\n",
        "                    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "                    temperature=0.7\n",
        "                )\n",
        "            return response.choices[0].message.content\n",
        "        except Exception as e:\n",
        "            if attempt == max_retries - 1:\n",
        "                return f\"ERROR: {str(e)}\"\n",
        "            time.sleep(2 ** attempt)  # Exponential backoff\n",
        "\n",
        "\n",
        "def extract_json_from_response(response: str) -> Dict:\n",
        "    \"\"\"Extract JSON from LLM response, handling markdown code blocks.\"\"\"\n",
        "    try:\n",
        "        # Try direct JSON parse first\n",
        "        return json.loads(response)\n",
        "    except:\n",
        "        # Try to find JSON in markdown code blocks\n",
        "        import re\n",
        "        json_match = re.search(r'```(?:json)?\\s*(\\{.*?\\})\\s*```', response, re.DOTALL)\n",
        "        if json_match:\n",
        "            return json.loads(json_match.group(1))\n",
        "        # Try to find JSON without code blocks\n",
        "        json_match = re.search(r'\\{.*\\}', response, re.DOTALL)\n",
        "        if json_match:\n",
        "            return json.loads(json_match.group(0))\n",
        "        raise ValueError(\"Could not extract JSON from response\")\n",
        "\n",
        "\n",
        "print(\"âœ“ Helper functions defined\")"
      ],
      "metadata": {
        "id": "hKvnneQ8vYIO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "615c4f6f-bf37-4e01-ca6b-63ee803dbc2b"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ“ Helper functions defined\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# CELL 5: Stage 1 - Generate Arguments from Different Personas\n",
        "# =============================================================================\n",
        "\"\"\"\n",
        "Each model generates arguments from each persona's perspective.\n",
        "\"\"\"\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"STAGE 1: GENERATING ARGUMENTS FROM DIFFERENT PERSONAS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "arguments = {}\n",
        "\n",
        "for model in MODELS:\n",
        "    print(f\"\\nðŸ¤– Querying {model}...\")\n",
        "    arguments[model] = {}\n",
        "\n",
        "    for persona_name, persona_info in PERSONAS.items():\n",
        "        prompt = f\"\"\"\n",
        "Topic: {TOPIC}\n",
        "\n",
        "Role: {persona_info['description']}\n",
        "\n",
        "{persona_info['instruction']}\n",
        "\n",
        "Provide 3-5 key arguments in a clear, structured format. Be specific and persuasive.\n",
        "Keep your response concise (max 200 words).\n",
        "\"\"\"\n",
        "\n",
        "        print(f\"  â†’ Getting {persona_name} perspective...\")\n",
        "        response = call_llm(model, prompt)\n",
        "        arguments[model][persona_name] = response\n",
        "        time.sleep(0.5)  # Rate limiting\n",
        "\n",
        "print(\"\\nâœ“ Stage 1 complete! All arguments collected.\")\n",
        "print(f\"Total responses: {len(MODELS) * len(PERSONAS)}\")"
      ],
      "metadata": {
        "id": "hnIrt-sCvaQF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67557736-be4b-44eb-b839-6f7612417306"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "STAGE 1: GENERATING ARGUMENTS FROM DIFFERENT PERSONAS\n",
            "================================================================================\n",
            "\n",
            "ðŸ¤– Querying gpt-4o-mini...\n",
            "  â†’ Getting optimist perspective...\n",
            "  â†’ Getting skeptic perspective...\n",
            "  â†’ Getting pragmatist perspective...\n",
            "\n",
            "ðŸ¤– Querying claude-3-5-haiku-20241022...\n",
            "  â†’ Getting optimist perspective...\n",
            "  â†’ Getting skeptic perspective...\n",
            "  â†’ Getting pragmatist perspective...\n",
            "\n",
            "ðŸ¤– Querying gemini-1.5-flash...\n",
            "  â†’ Getting optimist perspective...\n",
            "  â†’ Getting skeptic perspective...\n",
            "  â†’ Getting pragmatist perspective...\n",
            "\n",
            "âœ“ Stage 1 complete! All arguments collected.\n",
            "Total responses: 9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# CELL 6: Display Arguments\n",
        "# =============================================================================\n",
        "\"\"\"\n",
        "View the arguments generated by each model/persona combination.\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"COLLECTED ARGUMENTS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "for model in MODELS:\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"MODEL: {model}\")\n",
        "    print(f\"{'='*80}\")\n",
        "\n",
        "    for persona_name in PERSONAS.keys():\n",
        "        print(f\"\\n--- {persona_name.upper()} ---\")\n",
        "        print(arguments[model][persona_name])\n",
        "        print()"
      ],
      "metadata": {
        "id": "rYDgAxQtvic9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7179e1e-d6b7-4c82-fb19-c52b6194988d"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "COLLECTED ARGUMENTS\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "MODEL: gpt-4o-mini\n",
            "================================================================================\n",
            "\n",
            "--- OPTIMIST ---\n",
            "ERROR: litellm.RateLimitError: RateLimitError: OpenAIException - You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.\n",
            "\n",
            "\n",
            "--- SKEPTIC ---\n",
            "ERROR: litellm.RateLimitError: RateLimitError: OpenAIException - You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.\n",
            "\n",
            "\n",
            "--- PRAGMATIST ---\n",
            "ERROR: litellm.RateLimitError: RateLimitError: OpenAIException - You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.\n",
            "\n",
            "\n",
            "================================================================================\n",
            "MODEL: claude-3-5-haiku-20241022\n",
            "================================================================================\n",
            "\n",
            "--- OPTIMIST ---\n",
            "ERROR: litellm.BadRequestError: AnthropicException - {\"type\":\"error\",\"error\":{\"type\":\"invalid_request_error\",\"message\":\"Your credit balance is too low to access the Anthropic API. Please go to Plans & Billing to upgrade or purchase credits.\"},\"request_id\":\"req_011CVTcFgCB3CV5czrT4FN1d\"}\n",
            "\n",
            "\n",
            "--- SKEPTIC ---\n",
            "ERROR: litellm.BadRequestError: AnthropicException - {\"type\":\"error\",\"error\":{\"type\":\"invalid_request_error\",\"message\":\"Your credit balance is too low to access the Anthropic API. Please go to Plans & Billing to upgrade or purchase credits.\"},\"request_id\":\"req_011CVTcFxcPnw7VFVBZDmZ4L\"}\n",
            "\n",
            "\n",
            "--- PRAGMATIST ---\n",
            "ERROR: litellm.BadRequestError: AnthropicException - {\"type\":\"error\",\"error\":{\"type\":\"invalid_request_error\",\"message\":\"Your credit balance is too low to access the Anthropic API. Please go to Plans & Billing to upgrade or purchase credits.\"},\"request_id\":\"req_011CVTcGFLy1WiKDkgW1XfeV\"}\n",
            "\n",
            "\n",
            "================================================================================\n",
            "MODEL: gemini-1.5-flash\n",
            "================================================================================\n",
            "\n",
            "--- OPTIMIST ---\n",
            "ERROR: 'NoneType' object does not support item assignment\n",
            "\n",
            "\n",
            "--- SKEPTIC ---\n",
            "ERROR: 'NoneType' object does not support item assignment\n",
            "\n",
            "\n",
            "--- PRAGMATIST ---\n",
            "ERROR: 'NoneType' object does not support item assignment\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# CELL 7: Stage 2 - Create Comparison Tables\n",
        "# =============================================================================\n",
        "\"\"\"\n",
        "Ask each model to analyze all arguments and create structured comparison tables.\n",
        "\"\"\"\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"STAGE 2: CREATING COMPARISON TABLES\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Compile all arguments for analysis\n",
        "all_arguments_text = \"\\n\\n\".join([\n",
        "    f\"=== {model} - {persona} ===\\n{arg}\"\n",
        "    for model, personas in arguments.items()\n",
        "    for persona, arg in personas.items()\n",
        "])\n",
        "\n",
        "comparison_prompt = f\"\"\"\n",
        "Topic: {TOPIC}\n",
        "\n",
        "Here are arguments from multiple perspectives:\n",
        "\n",
        "{all_arguments_text}\n",
        "\n",
        "Analyze these arguments and create a structured assessment with these criteria:\n",
        "\n",
        "1. VALENCE: Categorize each major point as Positive (Pro), Negative (Con), or Neutral\n",
        "2. IMPACT: Rate the potential impact as Small, Medium, or Large\n",
        "3. CONFIDENCE: Rate your confidence in the claim as Low, Medium, or High\n",
        "\n",
        "Provide your analysis in JSON format:\n",
        "{{\n",
        "    \"points\": [\n",
        "        {{\n",
        "            \"description\": \"brief point description\",\n",
        "            \"valence\": \"positive/negative/neutral\",\n",
        "            \"impact\": \"small/medium/large\",\n",
        "            \"confidence\": \"low/medium/high\"\n",
        "        }}\n",
        "    ]\n",
        "}}\n",
        "\n",
        "Identify 5-8 key points total. Return ONLY the JSON, no additional text.\n",
        "\"\"\"\n",
        "\n",
        "comparisons = {}\n",
        "\n",
        "for model in MODELS:\n",
        "    print(f\"\\nðŸ¤– Getting comparison from {model}...\")\n",
        "    response = call_llm(model, comparison_prompt)\n",
        "\n",
        "    try:\n",
        "        comparisons[model] = extract_json_from_response(response)\n",
        "        print(f\"  âœ“ Extracted {len(comparisons[model]['points'])} points\")\n",
        "    except Exception as e:\n",
        "        print(f\"  âœ— Error parsing response: {e}\")\n",
        "        comparisons[model] = {\"points\": [], \"error\": str(e)}\n",
        "\n",
        "    time.sleep(0.5)\n",
        "\n",
        "print(\"\\nâœ“ Stage 2 complete! Comparison tables created.\")\n"
      ],
      "metadata": {
        "id": "qSRTJ74Bvquq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c342816-bf2e-4de5-dcc7-af2291f0e17a"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "STAGE 2: CREATING COMPARISON TABLES\n",
            "================================================================================\n",
            "\n",
            "ðŸ¤– Getting comparison from gpt-4o-mini...\n",
            "  âœ— Error parsing response: Could not extract JSON from response\n",
            "\n",
            "ðŸ¤– Getting comparison from claude-3-5-haiku-20241022...\n",
            "  âœ— Error parsing response: 'points'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[92m23:37:53 - LiteLLM:ERROR\u001b[0m: vertex_llm_base.py:494 - Failed to load vertex credentials. Check to see if credentials containing partial/invalid information. Error: (\"Failed to retrieve http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/default/?recursive=true from the Google Compute Engine metadata service. Status: 404 Response:\\nb''\", <google.auth.transport.requests._Response object at 0x7b36ab327d10>)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/auth/compute_engine/credentials.py\", line 140, in _refresh_token\n",
            "    self._retrieve_info(request)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/auth/compute_engine/credentials.py\", line 107, in _retrieve_info\n",
            "    info = _metadata.get_service_account_info(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/auth/compute_engine/_metadata.py\", line 342, in get_service_account_info\n",
            "    return get(request, path, params={\"recursive\": \"true\"})\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/auth/compute_engine/_metadata.py\", line 267, in get\n",
            "    raise exceptions.TransportError(\n",
            "google.auth.exceptions.TransportError: (\"Failed to retrieve http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/default/?recursive=true from the Google Compute Engine metadata service. Status: 404 Response:\\nb''\", <google.auth.transport.requests._Response object at 0x7b36ab327d10>)\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/litellm/llms/vertex_ai/vertex_llm_base.py\", line 490, in get_access_token\n",
            "    _credentials, credential_project_id = self.load_auth(\n",
            "                                          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/litellm/llms/vertex_ai/vertex_llm_base.py\", line 120, in load_auth\n",
            "    self.refresh_auth(creds)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/litellm/llms/vertex_ai/vertex_llm_base.py\", line 252, in refresh_auth\n",
            "    credentials.refresh(Request())\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/auth/credentials.py\", line 365, in refresh\n",
            "    self._refresh_token(request)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/auth/compute_engine/credentials.py\", line 147, in _refresh_token\n",
            "    raise new_exc from caught_exc\n",
            "google.auth.exceptions.RefreshError: (\"Failed to retrieve http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/default/?recursive=true from the Google Compute Engine metadata service. Status: 404 Response:\\nb''\", <google.auth.transport.requests._Response object at 0x7b36ab327d10>)\n",
            "ERROR:LiteLLM:Failed to load vertex credentials. Check to see if credentials containing partial/invalid information. Error: (\"Failed to retrieve http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/default/?recursive=true from the Google Compute Engine metadata service. Status: 404 Response:\\nb''\", <google.auth.transport.requests._Response object at 0x7b36ab327d10>)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/auth/compute_engine/credentials.py\", line 140, in _refresh_token\n",
            "    self._retrieve_info(request)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/auth/compute_engine/credentials.py\", line 107, in _retrieve_info\n",
            "    info = _metadata.get_service_account_info(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/auth/compute_engine/_metadata.py\", line 342, in get_service_account_info\n",
            "    return get(request, path, params={\"recursive\": \"true\"})\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/auth/compute_engine/_metadata.py\", line 267, in get\n",
            "    raise exceptions.TransportError(\n",
            "google.auth.exceptions.TransportError: (\"Failed to retrieve http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/default/?recursive=true from the Google Compute Engine metadata service. Status: 404 Response:\\nb''\", <google.auth.transport.requests._Response object at 0x7b36ab327d10>)\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/litellm/llms/vertex_ai/vertex_llm_base.py\", line 490, in get_access_token\n",
            "    _credentials, credential_project_id = self.load_auth(\n",
            "                                          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/litellm/llms/vertex_ai/vertex_llm_base.py\", line 120, in load_auth\n",
            "    self.refresh_auth(creds)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/litellm/llms/vertex_ai/vertex_llm_base.py\", line 252, in refresh_auth\n",
            "    credentials.refresh(Request())\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/auth/credentials.py\", line 365, in refresh\n",
            "    self._refresh_token(request)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/auth/compute_engine/credentials.py\", line 147, in _refresh_token\n",
            "    raise new_exc from caught_exc\n",
            "google.auth.exceptions.RefreshError: (\"Failed to retrieve http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/default/?recursive=true from the Google Compute Engine metadata service. Status: 404 Response:\\nb''\", <google.auth.transport.requests._Response object at 0x7b36ab327d10>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ¤– Getting comparison from gemini-1.5-flash...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[92m23:37:55 - LiteLLM:ERROR\u001b[0m: vertex_llm_base.py:494 - Failed to load vertex credentials. Check to see if credentials containing partial/invalid information. Error: (\"Failed to retrieve http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/default/?recursive=true from the Google Compute Engine metadata service. Status: 404 Response:\\nb''\", <google.auth.transport.requests._Response object at 0x7b36a9c0ed80>)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/auth/compute_engine/credentials.py\", line 140, in _refresh_token\n",
            "    self._retrieve_info(request)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/auth/compute_engine/credentials.py\", line 107, in _retrieve_info\n",
            "    info = _metadata.get_service_account_info(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/auth/compute_engine/_metadata.py\", line 342, in get_service_account_info\n",
            "    return get(request, path, params={\"recursive\": \"true\"})\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/auth/compute_engine/_metadata.py\", line 267, in get\n",
            "    raise exceptions.TransportError(\n",
            "google.auth.exceptions.TransportError: (\"Failed to retrieve http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/default/?recursive=true from the Google Compute Engine metadata service. Status: 404 Response:\\nb''\", <google.auth.transport.requests._Response object at 0x7b36a9c0ed80>)\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/litellm/llms/vertex_ai/vertex_llm_base.py\", line 490, in get_access_token\n",
            "    _credentials, credential_project_id = self.load_auth(\n",
            "                                          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/litellm/llms/vertex_ai/vertex_llm_base.py\", line 120, in load_auth\n",
            "    self.refresh_auth(creds)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/litellm/llms/vertex_ai/vertex_llm_base.py\", line 252, in refresh_auth\n",
            "    credentials.refresh(Request())\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/auth/credentials.py\", line 365, in refresh\n",
            "    self._refresh_token(request)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/auth/compute_engine/credentials.py\", line 147, in _refresh_token\n",
            "    raise new_exc from caught_exc\n",
            "google.auth.exceptions.RefreshError: (\"Failed to retrieve http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/default/?recursive=true from the Google Compute Engine metadata service. Status: 404 Response:\\nb''\", <google.auth.transport.requests._Response object at 0x7b36a9c0ed80>)\n",
            "ERROR:LiteLLM:Failed to load vertex credentials. Check to see if credentials containing partial/invalid information. Error: (\"Failed to retrieve http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/default/?recursive=true from the Google Compute Engine metadata service. Status: 404 Response:\\nb''\", <google.auth.transport.requests._Response object at 0x7b36a9c0ed80>)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/auth/compute_engine/credentials.py\", line 140, in _refresh_token\n",
            "    self._retrieve_info(request)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/auth/compute_engine/credentials.py\", line 107, in _retrieve_info\n",
            "    info = _metadata.get_service_account_info(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/auth/compute_engine/_metadata.py\", line 342, in get_service_account_info\n",
            "    return get(request, path, params={\"recursive\": \"true\"})\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/auth/compute_engine/_metadata.py\", line 267, in get\n",
            "    raise exceptions.TransportError(\n",
            "google.auth.exceptions.TransportError: (\"Failed to retrieve http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/default/?recursive=true from the Google Compute Engine metadata service. Status: 404 Response:\\nb''\", <google.auth.transport.requests._Response object at 0x7b36a9c0ed80>)\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/litellm/llms/vertex_ai/vertex_llm_base.py\", line 490, in get_access_token\n",
            "    _credentials, credential_project_id = self.load_auth(\n",
            "                                          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/litellm/llms/vertex_ai/vertex_llm_base.py\", line 120, in load_auth\n",
            "    self.refresh_auth(creds)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/litellm/llms/vertex_ai/vertex_llm_base.py\", line 252, in refresh_auth\n",
            "    credentials.refresh(Request())\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/auth/credentials.py\", line 365, in refresh\n",
            "    self._refresh_token(request)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/auth/compute_engine/credentials.py\", line 147, in _refresh_token\n",
            "    raise new_exc from caught_exc\n",
            "google.auth.exceptions.RefreshError: (\"Failed to retrieve http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/default/?recursive=true from the Google Compute Engine metadata service. Status: 404 Response:\\nb''\", <google.auth.transport.requests._Response object at 0x7b36a9c0ed80>)\n",
            "\u001b[92m23:37:57 - LiteLLM:ERROR\u001b[0m: vertex_llm_base.py:494 - Failed to load vertex credentials. Check to see if credentials containing partial/invalid information. Error: (\"Failed to retrieve http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/default/?recursive=true from the Google Compute Engine metadata service. Status: 404 Response:\\nb''\", <google.auth.transport.requests._Response object at 0x7b36a9c0f6e0>)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/auth/compute_engine/credentials.py\", line 140, in _refresh_token\n",
            "    self._retrieve_info(request)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/auth/compute_engine/credentials.py\", line 107, in _retrieve_info\n",
            "    info = _metadata.get_service_account_info(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/auth/compute_engine/_metadata.py\", line 342, in get_service_account_info\n",
            "    return get(request, path, params={\"recursive\": \"true\"})\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/auth/compute_engine/_metadata.py\", line 267, in get\n",
            "    raise exceptions.TransportError(\n",
            "google.auth.exceptions.TransportError: (\"Failed to retrieve http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/default/?recursive=true from the Google Compute Engine metadata service. Status: 404 Response:\\nb''\", <google.auth.transport.requests._Response object at 0x7b36a9c0f6e0>)\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/litellm/llms/vertex_ai/vertex_llm_base.py\", line 490, in get_access_token\n",
            "    _credentials, credential_project_id = self.load_auth(\n",
            "                                          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/litellm/llms/vertex_ai/vertex_llm_base.py\", line 120, in load_auth\n",
            "    self.refresh_auth(creds)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/litellm/llms/vertex_ai/vertex_llm_base.py\", line 252, in refresh_auth\n",
            "    credentials.refresh(Request())\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/auth/credentials.py\", line 365, in refresh\n",
            "    self._refresh_token(request)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/auth/compute_engine/credentials.py\", line 147, in _refresh_token\n",
            "    raise new_exc from caught_exc\n",
            "google.auth.exceptions.RefreshError: (\"Failed to retrieve http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/default/?recursive=true from the Google Compute Engine metadata service. Status: 404 Response:\\nb''\", <google.auth.transport.requests._Response object at 0x7b36a9c0f6e0>)\n",
            "ERROR:LiteLLM:Failed to load vertex credentials. Check to see if credentials containing partial/invalid information. Error: (\"Failed to retrieve http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/default/?recursive=true from the Google Compute Engine metadata service. Status: 404 Response:\\nb''\", <google.auth.transport.requests._Response object at 0x7b36a9c0f6e0>)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/auth/compute_engine/credentials.py\", line 140, in _refresh_token\n",
            "    self._retrieve_info(request)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/auth/compute_engine/credentials.py\", line 107, in _retrieve_info\n",
            "    info = _metadata.get_service_account_info(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/auth/compute_engine/_metadata.py\", line 342, in get_service_account_info\n",
            "    return get(request, path, params={\"recursive\": \"true\"})\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/auth/compute_engine/_metadata.py\", line 267, in get\n",
            "    raise exceptions.TransportError(\n",
            "google.auth.exceptions.TransportError: (\"Failed to retrieve http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/default/?recursive=true from the Google Compute Engine metadata service. Status: 404 Response:\\nb''\", <google.auth.transport.requests._Response object at 0x7b36a9c0f6e0>)\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/litellm/llms/vertex_ai/vertex_llm_base.py\", line 490, in get_access_token\n",
            "    _credentials, credential_project_id = self.load_auth(\n",
            "                                          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/litellm/llms/vertex_ai/vertex_llm_base.py\", line 120, in load_auth\n",
            "    self.refresh_auth(creds)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/litellm/llms/vertex_ai/vertex_llm_base.py\", line 252, in refresh_auth\n",
            "    credentials.refresh(Request())\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/auth/credentials.py\", line 365, in refresh\n",
            "    self._refresh_token(request)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/auth/compute_engine/credentials.py\", line 147, in _refresh_token\n",
            "    raise new_exc from caught_exc\n",
            "google.auth.exceptions.RefreshError: (\"Failed to retrieve http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/default/?recursive=true from the Google Compute Engine metadata service. Status: 404 Response:\\nb''\", <google.auth.transport.requests._Response object at 0x7b36a9c0f6e0>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  âœ— Error parsing response: 'points'\n",
            "\n",
            "âœ“ Stage 2 complete! Comparison tables created.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# CELL 8: Display Comparison Tables\n",
        "# =============================================================================\n",
        "\"\"\"\n",
        "View the structured comparisons from each model.\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"COMPARISON TABLES\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "for model, data in comparisons.items():\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"MODEL: {model}\")\n",
        "    print(f\"{'='*80}\\n\")\n",
        "\n",
        "    if \"error\" in data:\n",
        "        print(f\"ERROR: {data['error']}\")\n",
        "        continue\n",
        "\n",
        "    df = pd.DataFrame(data['points'])\n",
        "    if not df.empty:\n",
        "        print(df.to_string(index=False))\n",
        "    else:\n",
        "        print(\"No points extracted\")\n",
        "    print()"
      ],
      "metadata": {
        "id": "GMVeKZTqvuao",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e794000d-4fc5-4599-a62c-dfe64c1c8068"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "COMPARISON TABLES\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "MODEL: gpt-4o-mini\n",
            "================================================================================\n",
            "\n",
            "ERROR: Could not extract JSON from response\n",
            "\n",
            "================================================================================\n",
            "MODEL: claude-3-5-haiku-20241022\n",
            "================================================================================\n",
            "\n",
            "ERROR: 'points'\n",
            "\n",
            "================================================================================\n",
            "MODEL: gemini-1.5-flash\n",
            "================================================================================\n",
            "\n",
            "ERROR: 'points'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# CELL 9: Stage 3 - Quantify Assessments\n",
        "# =============================================================================\n",
        "\"\"\"\n",
        "Convert categorical assessments to numerical scores (1-3 scale).\n",
        "\"\"\"\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"STAGE 3: QUANTIFYING ASSESSMENTS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Mapping for conversion\n",
        "VALENCE_MAP = {\"positive\": 3, \"neutral\": 2, \"negative\": 1}\n",
        "IMPACT_MAP = {\"large\": 3, \"medium\": 2, \"small\": 1}\n",
        "CONFIDENCE_MAP = {\"high\": 3, \"medium\": 2, \"low\": 1}\n",
        "\n",
        "quantified_data = []\n",
        "\n",
        "for model, data in comparisons.items():\n",
        "    if \"error\" in data or not data.get('points'):\n",
        "        continue\n",
        "\n",
        "    for point in data['points']:\n",
        "        quantified_data.append({\n",
        "            'model': model,\n",
        "            'description': point['description'],\n",
        "            'valence': point['valence'],\n",
        "            'valence_score': VALENCE_MAP.get(point['valence'].lower(), 2),\n",
        "            'impact': point['impact'],\n",
        "            'impact_score': IMPACT_MAP.get(point['impact'].lower(), 2),\n",
        "            'confidence': point['confidence'],\n",
        "            'confidence_score': CONFIDENCE_MAP.get(point['confidence'].lower(), 2)\n",
        "        })\n",
        "\n",
        "df_quantified = pd.DataFrame(quantified_data)\n",
        "\n",
        "print(f\"\\nâœ“ Quantified {len(df_quantified)} points\")\n",
        "print(\"\\nSample of quantified data:\")\n",
        "print(df_quantified.head(10).to_string(index=False))\n"
      ],
      "metadata": {
        "id": "rh9uWvNrv-Ze",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "604a99c9-3eb8-4795-995e-51e3672541d9"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "STAGE 3: QUANTIFYING ASSESSMENTS\n",
            "================================================================================\n",
            "\n",
            "âœ“ Quantified 0 points\n",
            "\n",
            "Sample of quantified data:\n",
            "Empty DataFrame\n",
            "Columns: []\n",
            "Index: []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# CELL 10: Stage 4 - Aggregate Scores\n",
        "# =============================================================================\n",
        "\"\"\"\n",
        "Calculate average scores across all models.\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STAGE 4: AGGREGATING SCORES\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "if not df_quantified.empty:\n",
        "    # Calculate overall averages\n",
        "    avg_scores = {\n",
        "        'avg_valence': df_quantified['valence_score'].mean(),\n",
        "        'avg_impact': df_quantified['impact_score'].mean(),\n",
        "        'avg_confidence': df_quantified['confidence_score'].mean()\n",
        "    }\n",
        "\n",
        "    print(\"\\nðŸ“Š OVERALL AVERAGE SCORES (1-3 scale):\")\n",
        "    print(f\"  Valence (1=Negative, 2=Neutral, 3=Positive): {avg_scores['avg_valence']:.2f}\")\n",
        "    print(f\"  Impact (1=Small, 2=Medium, 3=Large): {avg_scores['avg_impact']:.2f}\")\n",
        "    print(f\"  Confidence (1=Low, 2=Medium, 3=High): {avg_scores['avg_confidence']:.2f}\")\n",
        "\n",
        "    # Calculate by model\n",
        "    print(\"\\nðŸ“Š SCORES BY MODEL:\")\n",
        "    model_scores = df_quantified.groupby('model')[['valence_score', 'impact_score', 'confidence_score']].mean()\n",
        "    print(model_scores.to_string())\n",
        "\n",
        "    # Valence distribution\n",
        "    print(\"\\nðŸ“Š VALENCE DISTRIBUTION:\")\n",
        "    valence_counts = df_quantified['valence'].value_counts()\n",
        "    print(valence_counts.to_string())\n",
        "\n",
        "else:\n",
        "    print(\"âš ï¸  No quantified data available for aggregation\")"
      ],
      "metadata": {
        "id": "9fie0HXNwAd3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "435c90ec-9ac9-4c96-e515-84c8f438de9a"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "STAGE 4: AGGREGATING SCORES\n",
            "================================================================================\n",
            "âš ï¸  No quantified data available for aggregation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# CELL 11: Stage 5 - Voting and Conflict Resolution\n",
        "# =============================================================================\n",
        "\"\"\"\n",
        "Resolve disagreements through voting mechanisms.\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STAGE 5: CONFLICT RESOLUTION VIA VOTING\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "if not df_quantified.empty:\n",
        "    # Method 1: Majority Vote on Overall Stance\n",
        "    print(\"\\nðŸ—³ï¸  METHOD 1: MAJORITY VOTE ON OVERALL STANCE\")\n",
        "    valence_votes = df_quantified['valence'].value_counts()\n",
        "    majority_stance = valence_votes.idxmax()\n",
        "    print(f\"Majority stance: {majority_stance.upper()}\")\n",
        "    print(f\"Vote distribution: {valence_votes.to_dict()}\")\n",
        "\n",
        "    # Method 2: Weighted Vote (by confidence)\n",
        "    print(\"\\nðŸ—³ï¸  METHOD 2: CONFIDENCE-WEIGHTED VOTE\")\n",
        "    weighted_sum = (df_quantified['valence_score'] * df_quantified['confidence_score']).sum()\n",
        "    total_confidence = df_quantified['confidence_score'].sum()\n",
        "    weighted_avg = weighted_sum / total_confidence if total_confidence > 0 else 0\n",
        "\n",
        "    if weighted_avg > 2.3:\n",
        "        weighted_stance = \"POSITIVE\"\n",
        "    elif weighted_avg < 1.7:\n",
        "        weighted_stance = \"NEGATIVE\"\n",
        "    else:\n",
        "        weighted_stance = \"NEUTRAL\"\n",
        "\n",
        "    print(f\"Weighted stance: {weighted_stance}\")\n",
        "    print(f\"Weighted average: {weighted_avg:.2f}\")\n",
        "\n",
        "    # Method 3: Model Consensus\n",
        "    print(\"\\nðŸ—³ï¸  METHOD 3: MODEL CONSENSUS\")\n",
        "    model_stances = df_quantified.groupby('model')['valence_score'].mean()\n",
        "\n",
        "    consensus_threshold = 0.5  # How close models need to be\n",
        "    std_dev = model_stances.std()\n",
        "\n",
        "    if std_dev < consensus_threshold:\n",
        "        print(f\"âœ“ HIGH CONSENSUS (std dev: {std_dev:.2f})\")\n",
        "    else:\n",
        "        print(f\"âš ï¸  LOW CONSENSUS (std dev: {std_dev:.2f})\")\n",
        "\n",
        "    print(\"\\nModel stances:\")\n",
        "    for model, score in model_stances.items():\n",
        "        stance = \"POSITIVE\" if score > 2.3 else \"NEGATIVE\" if score < 1.7 else \"NEUTRAL\"\n",
        "        print(f\"  {model}: {score:.2f} ({stance})\")\n",
        "\n",
        "    # Method 4: High-Confidence Points Only\n",
        "    print(\"\\nðŸ—³ï¸  METHOD 4: HIGH-CONFIDENCE POINTS ONLY\")\n",
        "    high_conf = df_quantified[df_quantified['confidence_score'] >= 2.5]\n",
        "    if len(high_conf) > 0:\n",
        "        hc_avg_valence = high_conf['valence_score'].mean()\n",
        "        hc_stance = \"POSITIVE\" if hc_avg_valence > 2.3 else \"NEGATIVE\" if hc_avg_valence < 1.7 else \"NEUTRAL\"\n",
        "        print(f\"High-confidence stance: {hc_stance}\")\n",
        "        print(f\"Based on {len(high_conf)} high-confidence points\")\n",
        "    else:\n",
        "        print(\"No high-confidence points found\")\n",
        "\n",
        "else:\n",
        "    print(\"âš ï¸  No data available for voting\")\n"
      ],
      "metadata": {
        "id": "HL6PUz7EwHXO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0929a774-1cfd-47c0-8c32-36d589a8364b"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "STAGE 5: CONFLICT RESOLUTION VIA VOTING\n",
            "================================================================================\n",
            "âš ï¸  No data available for voting\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# CELL 12: Final Summary Report\n",
        "# =============================================================================\n",
        "\"\"\"\n",
        "Generate a comprehensive summary of the multi-agent analysis.\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"FINAL SUMMARY REPORT\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(f\"\\nðŸ“‹ TOPIC: {TOPIC}\")\n",
        "print(f\"ðŸ¤– MODELS CONSULTED: {len(MODELS)}\")\n",
        "print(f\"ðŸ‘¥ PERSONAS USED: {len(PERSONAS)}\")\n",
        "print(f\"ðŸ’¬ TOTAL POINTS ANALYZED: {len(df_quantified)}\")\n",
        "\n",
        "if not df_quantified.empty:\n",
        "    print(\"\\n\" + \"-\" * 80)\n",
        "    print(\"CONSENSUS FINDINGS:\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    print(f\"\\n1. Overall Sentiment: {majority_stance.upper()}\")\n",
        "    print(f\"   - {valence_votes[majority_stance]} out of {len(df_quantified)} points support this\")\n",
        "\n",
        "    print(f\"\\n2. Expected Impact: {df_quantified['impact'].mode().values[0].upper()}\")\n",
        "    print(f\"   - Average impact score: {avg_scores['avg_impact']:.2f}/3.0\")\n",
        "\n",
        "    print(f\"\\n3. Confidence Level: {df_quantified['confidence'].mode().values[0].upper()}\")\n",
        "    print(f\"   - Average confidence score: {avg_scores['avg_confidence']:.2f}/3.0\")\n",
        "\n",
        "    print(f\"\\n4. Model Agreement: {'HIGH' if std_dev < 0.5 else 'MODERATE' if std_dev < 1.0 else 'LOW'}\")\n",
        "    print(f\"   - Standard deviation: {std_dev:.2f}\")\n",
        "\n",
        "    print(\"\\n\" + \"-\" * 80)\n",
        "    print(\"RECOMMENDATION:\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    # Generate a final recommendation based on all factors\n",
        "    if weighted_avg > 2.3 and avg_scores['avg_confidence'] > 2.0:\n",
        "        recommendation = \"STRONGLY SUPPORT\"\n",
        "    elif weighted_avg > 2.0:\n",
        "        recommendation = \"CAUTIOUSLY SUPPORT\"\n",
        "    elif weighted_avg < 1.7 and avg_scores['avg_confidence'] > 2.0:\n",
        "        recommendation = \"STRONGLY OPPOSE\"\n",
        "    elif weighted_avg < 2.0:\n",
        "        recommendation = \"CAUTIOUSLY OPPOSE\"\n",
        "    else:\n",
        "        recommendation = \"REMAIN NEUTRAL / GATHER MORE DATA\"\n",
        "\n",
        "    print(f\"\\n{recommendation}\")\n",
        "    print(f\"\\nThis recommendation is based on:\")\n",
        "    print(f\"  â€¢ Weighted valence score: {weighted_avg:.2f}\")\n",
        "    print(f\"  â€¢ Average confidence: {avg_scores['avg_confidence']:.2f}\")\n",
        "    print(f\"  â€¢ Model consensus: {std_dev:.2f} std dev\")\n",
        "\n",
        "else:\n",
        "    print(\"\\nâš ï¸  Insufficient data for final recommendation\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"ANALYSIS COMPLETE\")\n",
        "print(\"=\" * 80)"
      ],
      "metadata": {
        "id": "mYTp5g8RwR5J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf2e3264-3dc3-46fd-ac32-9cf7ba015559"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "FINAL SUMMARY REPORT\n",
            "================================================================================\n",
            "\n",
            "ðŸ“‹ TOPIC: What is the tallest tree that ever existed?\n",
            "ðŸ¤– MODELS CONSULTED: 3\n",
            "ðŸ‘¥ PERSONAS USED: 3\n",
            "ðŸ’¬ TOTAL POINTS ANALYZED: 0\n",
            "\n",
            "âš ï¸  Insufficient data for final recommendation\n",
            "\n",
            "================================================================================\n",
            "ANALYSIS COMPLETE\n",
            "================================================================================\n"
          ]
        }
      ]
    }
  ]
}